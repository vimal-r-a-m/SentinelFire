# ğŸ”¥ SentinelFire â€” Weakly Supervised Wildfire Verification

This repository contains an endâ€‘toâ€‘end **machine learning systems project** that evaluates whether **spatialâ€“spectral learning using Sentinelâ€‘2 imagery** can improve **secondâ€‘stage verification and prioritization** of coarse wildfire alerts produced by MODIS, under **weak supervision**.

The work is framed as a **verification and riskâ€‘ranking problem**, not wildfire prediction or burnâ€‘severity estimation.

---

## ğŸ“Œ Oneâ€‘Sentence Summary

> **Given a MODIS fire alert, can highâ€‘resolution multispectral learning better rank how risky that alert looks compared to simple ruleâ€‘based spectral indices?**

---

## ğŸš¨ What This Project Is â€” and Is Not

### âœ… This project **does**
- Treat MODIS fire alerts as **weak supervision**
- Evaluate CNNs as **secondâ€‘stage verification models**
- Compare learning against **singleâ€‘date NBR heuristics**
- Perform **ablation and disagreement analysis**
- Emphasize **ranking, recall, and robustness**
- Propose a **costâ€‘aware cascaded deployment strategy**

### âŒ This project **does NOT**
- Predict wildfire ignition
- Estimate burn severity (no dNBR)
- Replace MODIS detection
- Claim pixelâ€‘level ground truth
- Optimize for accuracy, F1, or PRâ€‘AUC

---

## ğŸ—‚ Dataset

The project uses the **Sen2Fire** benchmark dataset.

**Inputs**
- Sentinelâ€‘2 multispectral patches (512 Ã— 512, 10 m)
- 12 Sentinelâ€‘2 bands (B1â€“B12)
- Optional Sentinelâ€‘5P aerosol index

**Labels**
- MODIS MOD14A1 V6.1 daily fire product

**Splits (geographically disjoint)**
- Train: scene1, scene2
- Validation: scene3
- Test: scene4

This split prevents spatial and temporal leakage.

---

## âš ï¸ Weak Supervision Assumptions (Critical)

MODIS fire labels are:
- Coarse (1 km resolution)
- Binary
- Temporally misaligned with Sentinelâ€‘2
- Not pixelâ€‘accurate

Consequences:
- Fire occupies a small fraction of each patch
- Many apparent false positives may be label noise
- Some visually burned regions are unlabeled

ğŸ‘‰ This motivates **patchâ€‘level classification** and **rankingâ€‘based evaluation**, not pixelâ€‘level segmentation.

---

## ğŸ§  Problem Formulation

**Task:** Patchâ€‘level binary classification

> â€œDoes this MODISâ€‘flagged region exhibit fireâ€‘like spatialâ€“spectral evidence in Sentinelâ€‘2 imagery?â€

**Why patchâ€‘level?**
- Matches label fidelity
- Avoids circular refinement
- Aligns with MODIS alert semantics

---

## ğŸ§ª Baselines and Models

### Ruleâ€‘Based Baseline
- **Singleâ€‘date NBR thresholding**
- Represents common GIS heuristics
- Used as a **baseline comparator**, not ground truth

> dNBR is intentionally not used because it requires reliable preâ€‘ and postâ€‘fire imagery, which is unavailable in this benchmark.

---

### Learned Models

1. **Spectralâ€‘only Logistic Regression**
   - Spatially averaged band values
   - Tests whether spatial context matters

2. **Lightweight CNN (Patchâ€‘Level)**

```
Conv â†’ ReLU â†’ Pool
Conv â†’ ReLU â†’ Global Average Pool â†’ Linear
```

**Why this architecture?**
- Minimal capacity to avoid fitting label noise
- Convolutions capture local spatial patterns
- Global Average Pooling aligns with coarse supervision
- Patchâ€‘level output matches MODIS semantics

---

## ğŸ“Š Evaluation Metrics

### Primary Metric â€” AUROC
- Thresholdâ€‘independent
- Robust to class imbalance
- Suitable for **ranking / risk prioritization**

### Reported for interpretation
- Recall (fire): safetyâ€‘critical
- Precision (fire): diagnostic only

### Explicitly not used
- Accuracy (misleading under imbalance)
- F1 score (thresholdâ€‘sensitive under weak labels)
- PRâ€‘AUC (precision unreliable due to label noise)

---

## ğŸ” Disagreement Analysis (Core Contribution)

Rather than relying only on aggregate metrics, the project analyzes **where models disagree**:

| Case | Interpretation |
|----|----|
| CNN âœ” / NBR âœ˜ | CNN recovers fires missed by brittle index rules |
| CNN âœ˜ / NBR âœ” | CNN suppresses ruleâ€‘based false positives |
| Both âœ” | Easy, obvious fire cases |
| Both âœ˜ | Clear nonâ€‘fire cases |

**Key findings**
- CNN is more robust under smoke, partial burns, and mixed land cover
- NBR frequently triggers on bare soil and seasonal vegetation stress
- Most CNN false negatives correspond to sparse or ambiguous MODIS labels

This analysis explains *why* learning helps, not just *that* it helps.

---

## ğŸ§© Ablation Studies

The following ablations isolate what contributes to performance:

| Ablation | Observation |
|-------|-------------|
| Spectralâ€‘only | Performance drops â†’ spatial context matters |
| RGB only | Weak performance |
| NIR + SWIR | **Best AUROC (~0.83)** |
| All bands | Performance degrades due to noise |
| + Aerosol | Recall increases, discrimination decreases |

**Conclusion:**  
Targeted spectral selection and spatial context matter more than feature quantity.

---

## ğŸ— System Design Perspective

### Evaluation Setup (Used in This Project)

```
MODIS alert
   â†“
Sentinelâ€‘2 patch
   â†“
[NBR]     [CNN]     (parallel evaluation)
```

Used to:
- Understand failure modes
- Justify ML usage
- Avoid evaluation bias

---

### Proposed Production Deployment (Future Work)

```
MODIS alert
   â†“
NBR (cheap rule)
   â†“
NBRâ€‘negative cases
   â†“
CNN (selective verifier)
```

This mirrors **cascaded decision systems** used in spam filtering and fraud detection:
- Rules handle easy cases
- ML corrects systematic failures
- Balanced performance vs compute cost

---

## ğŸ“ˆ Results Summary

- CNN (NIR + SWIR): **AUROC â‰ˆ 0.83**
- Outperforms NBR thresholding and spectralâ€‘only baselines
- Gains come from spatial context and burnâ€‘sensitive bands

---

## ğŸ““ Notebook

All experiments are implemented in:

```
SentinelFire_Weakly_Supervised_Wildfire_Verification.ipynb
```

---

## ğŸ›  How to Run

```bash
git clone <repo-url>
cd <repo>
pip install -r requirements.txt
jupyter notebook SentinelFire_Weakly_Supervised_Wildfire_Verification.ipynb
```

---

## ğŸ“Œ Resumeâ€‘Ready Highlights

- Built a weakly supervised wildfire alert verification system using Sentinelâ€‘2 imagery and MODIS fire products, achieving AUROC â‰ˆ 0.83 on heldâ€‘out fire events.
- Demonstrated that lightweight spatial CNNs using NIRâ€“SWIR bands outperform NBR thresholding and spectralâ€‘only baselines under noisy supervision.
- Performed ablation and disagreement analysis to identify where learning improves robustness over ruleâ€‘based heuristics.
- Proposed a costâ€‘aware cascaded deployment strategy where ML selectively corrects ruleâ€‘based failures.

---

## âš–ï¸ Disclaimer

This project is for **research and educational purposes only** and is not an operational wildfire detection system.
